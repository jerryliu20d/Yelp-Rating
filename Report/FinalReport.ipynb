{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are going to analyse the data fetch from Yelp. Yelp is an app that collects information about restaurants and other business. Its recommendation system will choose the best for customers according to history reviews. We now fetch over 1 million reviews and over 50,000 companies information. In our project, we only focus on the **Chinese restaurant**.\n",
    "\n",
    "We have two main goals. Firstly, we wanted to provide some practical advice for the business owner. On one hand, we provided suggestions according to the business attributes. We first extracted business attributes and sorted them according to the feature importance in Light Gradient Boost (LGB). We discussed whether the first few attributes had any influence on business average stars with ANOVA. On the other hand, we analyzed customer reviews. We vectorized preprocessed words. Then group them by 5 features (sanitation, food, waiting time, service, price) by Cosine similarity. Then we count the frequency of each group in each review and normalized them. Finally, we use Earth Mover Distance to measure how these features affect the dining experience. We randomly pick some Chinese restaurants and provide some suggestions by the analysis above.\n",
    "\n",
    "Secondly, we wanted to predict the reviews' star according to the review texts. We tried many methods and finally, we choose the LSTM model. It did a great job and our final RMSE is 0.59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting Chinese restaurants by searching for the word 'Chinese' in the column 'categories' in  business data, there are 3557 businesses remained, along with 209897 reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the category according to the comma. Then pick out the Chinese restaruants. Then for other attributes, we denote all missing value like 'N/A', 'None', empty as one distinct value 'None'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the words according to the following steps:\n",
    "1. Remove the reviews that contain non-english characters\n",
    "2. Splite the words according to the white space and other punctuations\n",
    "3. Change the upper-case into lower-case\n",
    "4. Expand the common abbreviation like: wouldn't → would, not\n",
    "5. Remove punctuations\n",
    "6. Delete stopping words, but keep some words like: not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualization of Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data cleaning, we first looked into working times of the restaurants. However, restaurants with different ratings apprear to have similar average working times per week. Then barplots of average stars in different categories for specific atttributes were drawn. Some interesting patterns were discovered, such as average rating for dinner restaurants tend to be high but opposite for breakfast, and noise level also has an influence on the ratings.\n",
    "\n",
    "The 'NoiseLevel' example: <img src=\"NoiseLevel.png\", width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visulization of Reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For review texts, we first drew some wordclouds (words are ordered by appearance frequencies):\n",
    "<img src=\"Wordcloud.png\", width = 700> \n",
    "Obviously, \"food\", \"service\", \"time\" etc were frequently mentioned in the reviews. So we naturally took an assumption that these are aspects that have strong impact on stars.\n",
    "\n",
    "Secondly, we calculated word frequencies of various kinds of food in each star and visualized them by barplots. It turned out that beef, shrimp, crab and eggplant are the most popular ingredients and spicy is the most popular favor. On the other hand, this indicates that food is an important feature associated with ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Suggestions for Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Attributes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the missing data, we thought they also provide some information. Usually speaking, the missing value means the restaurant does not have such equipment or service. For example, if they do not provide the information about the wifi, they may not have accessible wifi. The restaurant owner may forget to provide such information. In this situation, this feature may not be an advantage of their service, otherwise, they will certainly propagate it to attract customers. And finally, the missing data may be caused by the Yelp database. But it is not the main reason. We cannot distinguish them from others. So we can safely ignore it. Our final decision was to denote them as specific categories called 'None'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing the attributes, we used Light Gradient Boost to rank the feature importance. Light Gradient Boost is a tree-based method that can do classification or regression. Intuitively speaking, Light Gradient Boost will first randomly pick one feature and split the tree nodes and then find the best split rule to minimize the regulized objective function. Then it searches all possible features to find the one with the minimum objective function. And the tree grows until the improvement of the objective function does not reach the benchmark. We repeat that process several times. And finally, we combine all leaves of these trees together (usually we sum them up).\n",
    "\n",
    "From the above algorithm, we can see that the Light Gradient Boost will certainly first split node with the most important feature. And for those features have a high correlation with the selected feature, Light Gradient Boost may not choose them again. It provides us with a method to rank the importance of the attributes.\n",
    "\n",
    "Following is the barplot of importance levels for attributes: <img src=\"FeatureImportance.png\", width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we applied ANOVA on the selected 88 attributes. P-values, feature importance levels, and non-missing sample sizes of attributes are recorded in file *'Tables/Attributes_ANOVA.csv'* on github repository. Only 5 attributes (**’Noise Level‘, ’Caters‘, ’HasTV‘, ’Restaurants Reservations‘ and 'Outdoor Seating'**) passed both the cutoff of greater or equal to 50 in importance level and less or equal to 0.05/88=0.00057 in ANOVA p-values. So we decided to mainly focus on these five attributes in the suggestion part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Review Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, We use word2vec model to vectorized the words. More specifically, we use Skip Gram model. To introduce the Skip Gram, we need to define the context and target word first. For each target word, we d                                  efine the previous and following 5 words as the context of the target word, i.e. window_size = 5. Then we train the Skip Gram with target word as input and the context word as output. Generally speaking, Skip Gram can predict the context word according to the target words. But here we use it to find similar words.\n",
    "\n",
    "We extract the word vectors and then use cosine similarity to find the correlated words. The cosine similarity is defined as:\n",
    "$$\\frac{x*y}{\\Vert x\\Vert\\Vert y\\Vert}$$\n",
    ", where x, y are vectors of two different words. We read a few reviews and search for some background information. And finally we decided to analyze the following five aspects: sanitation, food, waiting for time, service, price. We first pick some words and use cosine similarity to find similar words. We manually check the selected words and repeat that process several times. You can find these words in the data folder.\n",
    "\n",
    "Then we count these bag of words frequency in each review and normalized them with word length. Then we group these data by the stars. We define stars 1 or 2 as negative and 4 or 5 as positive. Then we use Earth Mover’s Distance (EMD) to measure the difference of the distribution of positive and negative frequency. The reason why we did not use KL divergence is that it will 'explode' in some situation (think of KL divergence between N(0, $\\varepsilon^3$) and N($\\varepsilon$, $\\varepsilon^3$) when $\\varepsilon\\to 0$. Also, it is not a measure of distance (strictly speaking) because it is not symmetric. But Earth Mover’s Distance can make a remedy of it. See more details [here](https://vincentherrmann.github.io/blog/wasserstein/). And finally, we sort these 5 features according to the EMD and determine whether it is an advantage or disadvantage. Then we provide some related suggestions on it.\n",
    "<img src=\"example2.png\", width=\"500\" height=\"50\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Example Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly pick a chinese restaraunt who has more than 20 reviews. Here we take the restaurant 'Dragon Wok' as an example. \n",
    "\n",
    "We first analyze its attribute. We pick the first 6 attributes according to the feature importance from Light Gradient Boost. The Noiselevel is average but people prefer to eat in a comparatively quiet environment. We suggest them try to lower down the noise from the kitchen. As for the other attributes, we thought they did well. They provide restaurant delivery. They have caters. Alcohol is not allowed (we thought traditional chinese food didnot taste good with wine or Liquor). \n",
    "\n",
    "|-|Service|Food|Sanitation|Price|Time|\n",
    "|-|--|--|--|--|--|\n",
    "|EMD|3.72|3.82|9.34|10.23|12.20|  \n",
    "\n",
    "                                        The EMD of each review feature\n",
    "Next, we analyze the reviews of this restaurant. We found that **time** and **price** have comparatively large EMD, which means the word frequency distribution between the positive and negative reviews have significant difference (compare with the other review features). However, the barplot shows that the word frequency of positive and negative reviews have larger difference between the **price**. As we said in chapter 4.2, EMD measure the difference between two distribution. The **time** feature has greater EMD but lower difference in mean value. That means the positive and negative distribution must have some difference in the tail behavior. The following figure shows that the distribution of the negative reviews have heavier tail. The X-axis is the normalized word frequency for each review. The larger x value means that the customer has stronger positive/negative feelings. For negative reviews, we should put more attention on the tail behavior.\n",
    "<table>\n",
    "    <td>\n",
    "        <img src=\"example1.png\", width=\"500\" height=\"50\"> <br>\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"WordFreqTime.png\", width=350>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predict Review Stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the review stars only based on the preprocessed review data. Here we donot use the usual statistical model. Because the Natural Language Problem (NLP) has special characteristics. The neighboring words has strong connection. The classical classification or regression model can not handle it well. But Long Short-Term Memory (LSTM) take the sequence of the words into consideration! LSTM is based on Recurrent Neural Network (RNN). Although RNN can also handle the NLP, it has a fatal disadvantage. It cannot connect the shallower layers with the deeper layers very well. But LSTM use 'three gates' structure to solve this problem. The principle of LSTM is complicated. We will not discuss the algorithm in details.\n",
    "\n",
    "We train the model with following steps:\n",
    "\n",
    "1. We first transfer the preprocessed words into integers. Each word has a distinct integer number.\n",
    "2. Embed the sentences into word vectors. Each vector has length = 128, which is a determined hyperparameter.\n",
    "3. Train the LSTM model with batch size = 32 and epoch = 1. And get the 50 parameters as output.\n",
    "4. Add Pooling Layer to filter the features out.\n",
    "5. Use drop out layer to strength the training effect.\n",
    "6. Add Dense layer with Rectified Linear Unit (Relu) activation function. It can avoid the gradient vanish problem.\n",
    "7. Add Dense layer with linear activation function and get the predicted stars.\n",
    "\n",
    "Finally, our RMSE on test data is 0.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Contribution\n",
    "Lijie Liu: Introduction, Preprocess, Missing Data, Light Gradient Boost, Example Illustartion, Prediction  \n",
    "Ning Shen: ANOVA, Preliminary Analysis  \n",
    "Xiangan Zhang: Example Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Contribution\n",
    "Lijie Liu: Word Preprocess, Word Similarity, slides  \n",
    "Ning Shen: ANOVA, Preliminary Analysis, slides  \n",
    "Xiangan Zhang: LSTM prediction, Light Gradient Boost, EMD, slides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
